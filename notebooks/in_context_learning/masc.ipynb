{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea00ed30-93ab-42fb-9297-df1958626a9e",
   "metadata": {},
   "source": [
    "# Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30143073-b4f4-4680-af0a-6b22a7605a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import nltk\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5221b71-64f0-42cb-bd0a-cf021b9c5023",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "detokenize = TreebankWordDetokenizer().detokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05b1260c-1020-49ec-85ce-52e75bf62915",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = nltk.corpus.masc_tagged.fileids()\n",
    "corpus = [s for f in files for s in nltk.corpus.masc_tagged.sents(f)]\n",
    "corpus = [s for s in corpus if 4 < len(s) < 11]\n",
    "corpus = [[w for w in s if w not in \":.!?'[]\\\"\\'“”\"] for s in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0061c3f-87ce-4e68-ae2f-f75d9ca8f3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Out filtered corpus contains 8767 sentences'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Out filtered corpus contains {} sentences'.format(len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec95f62-c6a6-4423-8692-4ff02ab03f85",
   "metadata": {},
   "source": [
    "Some useful functions for categorizing sentences, quickly getting a handle on those sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a356c5f-9e4d-4835-80d7-0d25b7bee542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_present_in_sentence(s):\n",
    "    tagged = nltk.pos_tag(s)\n",
    "    tags = [w[1] for w in tagged]\n",
    "    return tags\n",
    "\n",
    "def display_sampling(corpus, condition, k = 5, seed = None):\n",
    "    random.seed(seed)\n",
    "    to_display = random.choices([s for s in corpus if condition(s)], k = k)\n",
    "    for s in to_display:\n",
    "        print(detokenize(s))\n",
    "        \n",
    "def num_satisfying(corpus, condition):\n",
    "    return len([s for s in corpus if condition(s)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22e09cff-790f-442b-85c3-1bb97981699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conditions\n",
    "\n",
    "def is_true(s):\n",
    "    return True\n",
    "\n",
    "def ends_in_VBD(s):\n",
    "    \"Ends in a verb in the past tense\"\n",
    "    return nltk.pos_tag(s)[-1][1] in ['VBD']\n",
    "\n",
    "def contains_two(s):\n",
    "    return 'two' in s or 'Two' in s\n",
    "\n",
    "def contains_CD(s):\n",
    "    \"Contains a numerical reference\"\n",
    "    return \"CD\" in tags_present_in_sentence(s)\n",
    "\n",
    "def contains_sparrow(s):\n",
    "    return 'SPARROW' in s or 'Sparrow' in s or 'sparrow' in s\n",
    "\n",
    "def contains_ship(s):\n",
    "    return 'ship' in s or 'Ship' in s\n",
    "\n",
    "def contains_president(s):\n",
    "    return 'President' in s or 'president' in s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e97cc454-a18e-481f-be78-3082f4d7ffe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_satisfying(corpus, contains_president)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd517a9b-161e-4444-b50e-d87c4e897b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I supported the president\n",
      "Vice President Gore, on the environment\n",
      "Thank you, Mr. President\n",
      "Waking up with a new president\n",
      "Ralph Price 1st Vice President Cash Management Operations\n",
      "Waking up with a new president\n",
      "Waking up with a new president\n",
      "I supported the president\n",
      "Waking up with a new president\n",
      "Mr. President, new question, two minutes\n"
     ]
    }
   ],
   "source": [
    "display_sampling(corpus, contains_president, k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6721e078-4dfa-4897-a54b-b1c0b199b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [ends_in_VBD, contains_two, contains_CD, contains_sparrow, contains_ship, contains_president]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dfab76-3228-49cc-a4c0-16ae90966508",
   "metadata": {},
   "source": [
    "The above is helpful for playing around and getting sense of what our sentences look like. Now build a dataframe including the sentences and the result of our conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe0044d3-57c6-444e-b881-3ed7d7d37ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_condition_column(df, cond):\n",
    "    df[cond.__name__] = df.sentences.apply(cond)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e813fa5d-7b0c-4ce4-9412-c8378250b916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_corpus_and_conditions(corpus, conditions):\n",
    "    df = pd.DataFrame(pd.Series(corpus, name = 'sentences'))\n",
    "    for cond in conditions:\n",
    "        add_condition_column(df, cond)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "301b9642-9753-472f-a4c8-12d31e387197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>ends_in_VBD</th>\n",
       "      <th>contains_two</th>\n",
       "      <th>contains_CD</th>\n",
       "      <th>contains_sparrow</th>\n",
       "      <th>contains_ship</th>\n",
       "      <th>contains_president</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Only, the, subjects, tonight, and, the, quest...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[The, format, tonight, is, that, of, a, conver...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Good, evening, ,, Governor, Bush, ,, Vice, Pr...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Governor, Bush, ,, the, first, question, goes...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[I, have, ,, I, have]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences  ends_in_VBD  \\\n",
       "0  [Only, the, subjects, tonight, and, the, quest...        False   \n",
       "1  [The, format, tonight, is, that, of, a, conver...        False   \n",
       "2  [Good, evening, ,, Governor, Bush, ,, Vice, Pr...        False   \n",
       "3  [Governor, Bush, ,, the, first, question, goes...        False   \n",
       "4                              [I, have, ,, I, have]        False   \n",
       "\n",
       "   contains_two  contains_CD  contains_sparrow  contains_ship  \\\n",
       "0         False        False             False          False   \n",
       "1         False        False             False          False   \n",
       "2         False        False             False          False   \n",
       "3         False        False             False          False   \n",
       "4         False        False             False          False   \n",
       "\n",
       "   contains_president  \n",
       "0               False  \n",
       "1               False  \n",
       "2                True  \n",
       "3               False  \n",
       "4               False  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_corpus_and_conditions(corpus, conditions).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd8751a-21f7-4d8a-9c2d-77e08678231a",
   "metadata": {},
   "source": [
    "Now that we have a dataset (and a suite of tools for adding to the dataset), let's focus on getting the LLM to learn the condition in context and evaluate the LLM's learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "993ef2f1-910d-4f2f-b87b-3ee0b27db948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(df, cond, train_k = 5, test_k = 10, seed = None):\n",
    "\n",
    "    satisfying = df.loc[df[cond.__name__]== True]\n",
    "    try: train, test = train_test_split(satisfying, train_size = train_k, test_size = test_k, random_state = seed) #might not be enough samples to use test_k\n",
    "    except: train, test = train_test_split(satisfying, train_size = train_k, random_state = seed) #in which case, just use the remainder\n",
    "\n",
    "    not_satisfying = df.loc[df[cond.__name__]== False]\n",
    "    train = pd.concat([train, not_satisfying.sample(train.shape[0], random_state = seed)])\n",
    "    test = pd.concat([test, not_satisfying.sample(test.shape[0], random_state = seed)])\n",
    "    test = test.sample(frac = 1) #this randomizes the order, so that we don't show a bunch of True followed by a bunch of False\n",
    "\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "884e9647-ea23-40af-954d-7c8933874197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_test_prompt(train, test_s, cond):\n",
    "    p = \"Each of the following sentences is labelled True/False according to whether or not it follows a simple rule a child might guess:\\n\\n\"\n",
    "    \n",
    "    for s, label in zip(train['sentences'], train[cond.__name__]):\n",
    "        p += \"\\\"\" + detokenize(s) + \"\\\"; \" + str(label) +  \"\\n\"\n",
    "\n",
    "#    p += \"\\nFirst, state the condition as briefly as possible. Then, state 'Label: True' if the following sentence satisfies the condition and 'Label: False' otherwise.\\n\\n\"\n",
    "#    p += \"\\nReply 'Condition: <describe the secret condition briefly>' and on a new line write 'Condition satisfied: <True or False>' where the condition is satisfied if the next sentence satisfies the condition:\\n\\n\"\n",
    "    p += \"\\nYour job is to learn the rule, thinking carefully about the previous examples. Does the sentence:\\n\\n\"\n",
    "    p += \"\\\"\" + detokenize(test_s) + \"\\\"\"\n",
    "    p += \"\\n\\nfollow the rule? Give a 'Yes' or 'No' answer, with no explanation.\"\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834238bc-08c7-48a0-8c7b-01454ea69678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a4b4f3fd-8e4c-4e54-aa7d-6fda0758a88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(p):\n",
    "    response = client.completions.create(\n",
    "                model = \"gpt-3.5-turbo-instruct\",\n",
    "                prompt = p\n",
    "                )\n",
    "    return response.choices[0].text\n",
    "#    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "12b53ff7-ae58-499a-8cbb-a32ed90e8150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_icl(train, test, cond, verbose = True):\n",
    "    llm_labels_correctly = []\n",
    "    for i, test_s in enumerate(test.sentences):\n",
    "        p = single_test_prompt(train, test_s, cond)\n",
    "        resp = get_response(p)\n",
    "        llm_label = 'Yes' in resp or 'yes' in resp or 'YES' in resp\n",
    "        actual_label = test[cond.__name__].iloc[i]\n",
    "        if verbose:\n",
    "            if llm_label != actual_label:\n",
    "                print('On test sentence: '+ detokenize(test_s) + ' -- the LLM incorrectly evaluates: ' + str(llm_label))\n",
    "        llm_labels_correctly.append(llm_label == actual_label)\n",
    "    correct = 0\n",
    "    for label in llm_labels_correctly:\n",
    "        if label:\n",
    "            correct += 1\n",
    "    accuracy = correct/len(llm_labels_correctly)\n",
    "    print('LLM learning accuracy is: ' + str(accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96feab55-05a8-4098-b894-d2d10995e805",
   "metadata": {},
   "source": [
    "### Putting it all together now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "de478620-080b-48d9-923e-6cba53369f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_from_corpus_and_conditions(corpus, conditions)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d0ae5b67-c819-4f3b-86a1-b046c9ffe80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>ends_in_VBD</th>\n",
       "      <th>contains_two</th>\n",
       "      <th>contains_CD</th>\n",
       "      <th>contains_sparrow</th>\n",
       "      <th>contains_ship</th>\n",
       "      <th>contains_president</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Only, the, subjects, tonight, and, the, quest...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[The, format, tonight, is, that, of, a, conver...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Good, evening, ,, Governor, Bush, ,, Vice, Pr...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Governor, Bush, ,, the, first, question, goes...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[I, have, ,, I, have]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences  ends_in_VBD  \\\n",
       "0  [Only, the, subjects, tonight, and, the, quest...        False   \n",
       "1  [The, format, tonight, is, that, of, a, conver...        False   \n",
       "2  [Good, evening, ,, Governor, Bush, ,, Vice, Pr...        False   \n",
       "3  [Governor, Bush, ,, the, first, question, goes...        False   \n",
       "4                              [I, have, ,, I, have]        False   \n",
       "\n",
       "   contains_two  contains_CD  contains_sparrow  contains_ship  \\\n",
       "0         False        False             False          False   \n",
       "1         False        False             False          False   \n",
       "2         False        False             False          False   \n",
       "3         False        False             False          False   \n",
       "4         False        False             False          False   \n",
       "\n",
       "   contains_president  \n",
       "0               False  \n",
       "1               False  \n",
       "2                True  \n",
       "3               False  \n",
       "4               False  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "077cfc89-4bf8-44f2-be90-924a55e7a461",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cond = contains_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "81f23141-773b-45fa-b64e-2a1d085d32f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = split_train_test(df, test_cond, test_k = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "72c726e6-577f-48db-8e1a-bac8e012b6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = single_test_prompt(train, test.sentences.iloc[1], test_cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "1c4a2bf4-7689-4c44-8dcd-6f5fd40a6277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each of the following sentences is labelled True/False according to whether or not it follows a simple rule a child might guess:\n",
      "\n",
      "\"Mr. President, new question, two minutes\"; True\n",
      "\"Waiter Two teas\"; True\n",
      "\"six nine four eight seven three two two six two\"; True\n",
      "\"seven six four nine nine one two two six\"; True\n",
      "\"The candidate is allowed two minutes to answer\"; True\n",
      "\"Wyvern closes his eyes, and falls silent\"; False\n",
      "\"The know-how is the propagating organization\"; False\n",
      "\"Let Mr. Thompson speak\"; False\n",
      "\"Compare prices before you buy any significant item\"; False\n",
      "\"I expect your things are outside\"; False\n",
      "\n",
      "Your job is to learn the rule, thinking carefully about the previous examples. Does the sentence:\n",
      "\n",
      "\"I went into the room and sat down\"\n",
      "\n",
      "follow the rule? Give a 'Yes' or 'No' answer, with no explanation.\n"
     ]
    }
   ],
   "source": [
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "bf239c6f-ce2b-4bb1-b939-511adf000339",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.completions.create(\n",
    "                model = \"gpt-3.5-turbo-instruct\",\n",
    "                prompt = p\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "afa23970-0664-4e8c-8b36-bf1b3def041c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Yes\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "039cbfe2-a91f-4f26-b59c-1c5c1cb99302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On test sentence: I went into the room and sat down -- the LLM incorrectly evaluates: True\n",
      "On test sentence: Or Cantonese, as the case may be -- the LLM incorrectly evaluates: True\n",
      "On test sentence: He was being chased by two thugs -- the LLM incorrectly evaluates: False\n",
      "On test sentence: uh I first trained up two -- the LLM incorrectly evaluates: False\n",
      "On test sentence: Um, I didn't really have a favorite -- the LLM incorrectly evaluates: True\n",
      "On test sentence: Huge difference between the two -- the LLM incorrectly evaluates: False\n",
      "On test sentence: Agree with the previous two comments -- the LLM incorrectly evaluates: False\n",
      "On test sentence: There are two theories to arguing with women -- the LLM incorrectly evaluates: False\n",
      "On test sentence: We paid down the debt for two years -- the LLM incorrectly evaluates: False\n",
      "On test sentence: six two seven four two seven three eight O _ -- the LLM incorrectly evaluates: False\n",
      "On test sentence: for the two weeks that he's gone -- the LLM incorrectly evaluates: False\n",
      "On test sentence: But of course -- the LLM incorrectly evaluates: True\n",
      "On test sentence: Clones are people two -- the LLM incorrectly evaluates: False\n",
      "On test sentence: Closingthe browser window does NOT log you out properly -- the LLM incorrectly evaluates: True\n",
      "LLM learning accuracy is: 0.65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.65"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_icl(train, test, test_cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16140fde-1ed3-45d0-88f7-8951aa24bb23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "7eb89c23-3803-4fcd-9b7a-f60cc0ebb14e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "5dd486f6-dcde-45d1-a6b5-bf9d13eb9bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "b552470b-508a-45da-8983-2e558155e97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On test sentence: JACK SPARROW Good doggie -- the LLM incorrectly evaluates: False\n",
      "On test sentence: JACK SPARROW Technic - -- the LLM incorrectly evaluates: False\n",
      "On test sentence: JACK SPARROW What did the bird say -- the LLM incorrectly evaluates: False\n",
      "On test sentence: JACK SPARROW As you were, gents -- the LLM incorrectly evaluates: False\n",
      "On test sentence: JACK SPARROW Oh bugger -- the LLM incorrectly evaluates: False\n",
      "On test sentence: JACK SPARROW Hey -- the LLM incorrectly evaluates: False\n",
      "On test sentence: JACK SPARROW Wup -- the LLM incorrectly evaluates: False\n",
      "On test sentence: JACK SPARROW Save me -- the LLM incorrectly evaluates: False\n",
      "On test sentence: JACK SPARROW Pirate -- the LLM incorrectly evaluates: False\n",
      "LLM learning accuracy is: 0.55\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_icl(train, test, contains_sparrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "4fa3d7d9-94d5-4f1b-bc51-85cc7f1cadb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On test sentence: JACK SPARROW Pirate -- the LLM incorrectly evaluatesFalse\n",
      "On test sentence: JACK SPARROW You're a diamond, mate -- the LLM incorrectly evaluatesFalse\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_icl(train, test, contains_sparrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "7a7ab741-ff41-4a89-aea0-1bce31beb76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:three O _ three eight three three eight seven four\n",
      "1:How's that\n",
      "2:JACK SPARROW Pirate\n",
      "3:JACK SPARROW You're a diamond, mate\n"
     ]
    }
   ],
   "source": [
    "for i, s in enumerate(test.sentences):\n",
    "    print(str(i) + ':' + detokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ec1ee0-0b5c-4a15-8a06-4949594ead83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
